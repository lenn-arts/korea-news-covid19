# ---------------------------------------------
# KOREA NEWS COVID-19 RESPONSE EMOTIONALITY ANALYSIS 
# 
# Details:      We are analysing the articles of the English speaking newspaper in Korea (Korea Times, Korea Herald) from January 01 to May 23 2020 
#               containing the words "covid-19" or "coronavirus" for their positivity/negativitiy and their emotionality.
# Copyright:    Â© 2020 Lennart Schulze
# Author        Lennart Schulze, Group 5, Big Data Analysis Class 2020-01, Sungkyunkwan University
# 
# imports:      see packages
# Usage:        source(*filename*)
# History:      1.0.0 2020-05-30
# --------------------------------------------- 



# Initialization
rm(list=ls())
getwd()

# ==== PACKAGES ====

# # java, rJava installation
# install.packages("multilinguer")
# install.packages("ggplot2")
# library(multilinguer)
# install_jdk()
# 
# # Install dependency packages
# install.packages(c('stringr', 'hash', 'tau', 'Sejong', 'RSQLite', 'devtools'), type = "binary")
# 
# # Install github version
# install.packages("remotes")
# remotes::install_github('haven-jeon/KoNLP', upgrade = "never", INSTALL_opts=c("--no-multiarch"))


library(ggplot2) # plotting
# library(KoNLP)
library(dplyr) # data processing
library(plyr)
library(stringr) 
library(reshape2) # melting (necessary for plotting)



# ==== FILES ====

# - lexicons

# Stanford SocialSent
pos_neg_lex=read.table("./lexicons/worldnews.tsv", header=F)
head(pos_neg_lex)

# NRC Emotion Intensity Lexicon
emo_lex=read.table("./lexicons/NRC-Emotion-Intensity-Lexicon-v1.txt", sep="\t", header=T)


# - news articles 

kh_virus <- read.csv("./articles/articles_koreaherald_coronavirus_date.csv", header = T)
kh_covid <- read.csv("./articles/articles_koreaherald_covid19_date.csv", header = T)
kt_virus <- read.csv("./articles/articles_koreatimes_coronavirus.csv", header = T, sep=";")
kt_covid <- read.csv("./articles/articles_koreatimes_covid19.csv", header = T, sep=";")


# - prepare sample for coding

# write.csv(head(kh_virus, n=50), file="./articles/sample1.csv")
sample1 <- read.csv("./articles/sample1.csv", header = T)
sample1 <- sample1[-1] # drop first column

# write.csv(head(kh_covid, n=50), file="./articles/sample2.csv")
sample2 <- read.csv("./articles/sample2.csv", header = T)
sample2 <- sample2[-1] # drop first column



# ==== PREPROCESSING ====

# - date format change (for Korea Herald Articles)
date_transform = function(row){
  date = unlist(row["DATE"])
  date <- gsub(",","", date) %>% strsplit(" ")
  date <- unlist(date)
  year = date[3]
  m = match(date[1], month.abb) # lookup index from month.name constant
  m = sprintf("%02d", m) # extend to 2 digit
  day = date[2] 
  day = sprintf("%02d", as.numeric(day)) # extend to 2 digit
  date = paste(year, m, day, sep="-")
  row["DATE"] = date
  return(row)
}

# - write transformed data (only once)
# sample <- t(apply(sample, 1, date_transform))
# write.csv(t(apply(kh_virus, 1, date_transform)), file="./articles/articles_koreaherald_coronavirus_date.csv", row.names=F, sep=";")
# write.csv(t(apply(kh_covid, 1, date_transform)), file="./articles/articles_koreaherald_covid19_date.csv", row.names=F, sep=";")


# - aggreagate articles from all sources in single data frame
articles = rbind(kh_virus, kh_covid, kt_virus, kt_covid)
articles = distinct(articles) # eliminate copies (articles can contain both 'coronvirus', and 'covid-19')
articles <- articles[order(articles$DATE), ] # order by date
rownames(articles)<-NULL # row ids to default
articles <- articles[,c(1, 6)] # project only date and content
nrow(articles)


# - group articles per day
# toString(subset(articles, DATE=="2020-05-20")[,2])
# aggregates <- articles %>%  group_by(DATE) %>%  summarise(TXT = toString(CONTENT)) %>% ungroup()

aggregates <- aggregate(CONTENT~DATE, articles, toString) # aggregate articles per day on date column, concetanating string with "," seperator
# print(aggregates)


# - prepare results data frame, copying the dates
results = data.frame(DATE=aggregates[1])
emotions = c("anger", "anticipation", "disgust", "fear", "joy", "sadness", "surprise", "trust")
results[c("POS", "NEG", "OBJ", emotions)]=NA



# ==== ANALYSIS ====

# function score.sentiment 
score.sentiment = function(articles, .progress='none') { 
  # generate scores for each row in articles tables (represents one date)
  values_list = apply(articles,1,
                 function(row, r=results) 
                 { 
                   # PREPROCESSING OF TEXT
                   
                   # translate punctuation to spaces (sometimes full-stops used without space between sentences)
                   date = row[[1]]
                   article = row[2]
                   cat(date,"\n")
                   # select only latin sign words (excludes numbers (even '-19' in covid), control signs) (keeps the spaces for separation)
                   article = gsub("[^A-z|\\s]", " ", article)
                   # lowercase 
                   article = tolower(article)
                   # # remove digits
                   #article = gsub('\\d+', " ", article) 
                   article = gsub("[[:punct:]]", " ", article)
                   # delete words that have only 1 character
                   article = gsub("(\\s.\\s){1}", " ", article) 
                   # remove unnecessary spaces 
                   article = gsub("[ \t]{2,}", " ", article) 
                   article = gsub("^\\s+|\\s+$", "", article)
                  
                   # # remove control characters 
                   # article = gsub("[[:cntrl:]]", "", article) 
                   # # remove html links 
                   # article = gsub("http\\w+", " ", article) 
                   
                   # get words list
                   word.list = str_split(article , "\\s+") 
                   typeof(word.list)
                   words = unlist(word.list)
                   
                   
                   # ANALYSIS FOR 1) POSITITVY/NEGATIVITY, 2) EMOTIONS
                   pos_score = 0
                   neg_score = 0
                   pos_count = 0
                   neg_count = 0
                   
                   emo_values = vector(mode="numeric", length = length(emotions)) 
                   
                   # iterate over all words from aggreagated articles text of the current date
                   for (word in words){
                      pos_index = which(pos_neg_lex[,1]==word)
                      if (length(pos_index) > 0){
                        val = pos_neg_lex[pos_index, 2]
                        #print(val)
                        if (val >= 0){
                          pos_score = pos_score + val
                          pos_count = pos_count + 1
                        }
                        else if (val<0){
                          neg_score = neg_score + (-val)
                          neg_count = neg_count + 1
                        }
                      }
                      emo_occurances = which(emo_lex[,1]==word)
                      for (i in range(length(emo_occurances))){
                        emo_index = emo_occurances[i]
                        val = emo_lex[emo_index, 3]
                        category = emo_lex[emo_index, 2]
                        # print(word)
                        # print(match(category, emotions))
                        emo_values[match(category, emotions)] = emo_values[match(category, emotions)] + val
                      }
                   }
                   objectivity = 1 - ((pos_count+neg_count)/length(words)) # objectivity = ratio of subjective words to text length
                   positivity = pos_score/length(words)
                   negativity = neg_score/length(words)
                   pos_values = c(positivity, negativity, objectivity)
                   print(c(positivity, negativity, objectivity))
                   
                   print(emo_values)
                   print(emo_values/length(words))
                   emo_values = emo_values/length(words)
                   
                   values = c(pos_values, emo_values)
                   return(values)
                 })
  # matrix with scores for each day
  return(t(values_list)) 
} 

# ---> TURN ON to calculate (takes very long, thus saved in file)
words=score.sentiment(aggregates)
str(words)
results[,-1] = as.data.frame(words) # assign values to result data frame


write.csv(results, file="./articles/results_kh.csv", row.names=F, sep=";")
results = read.csv(file="./articles/results_kh.csv", row.names = NULL, header=T)
print(results)



# ==== PLOT ====

results_melt = melt(results[c(1,2:4)], id=c("DATE")) # melt values into single column data frame
head(results_melt)
ggplot(results_melt) + geom_line(aes(x=as.Date(DATE), y=value, colour=variable)) + 
  theme_bw() +
  scale_color_brewer(palette="Paired") +
  # theme(
  #   axis.text.x=element_text(angle = -90, vjust = 1, size = 6, hjust = 1)
  # ) +
  ggtitle("Emotionality in Korean News Articles on COVID-19") +
  xlab("Date") +
  ylab("Index")

  
  

# ==== OLD ====

#pos.words=scan("positive-words.txt", what="character", comment.char = ";")
#neg.words=scan("negative-words.txt", what="character", comment.char = ";")

#pratice


# #step1) Start file input code
# 
# ##When you want to put it as a txt file
# ##Example execution with obama.txt
# 
# conn=file("obama.txt",open="r")
# line=readLines(conn)
# for (i in 1:length(line)){
#   print(line[i])
# }
# close(conn)
# 
# #Notice before executing the sentence below: If the amount of txt is too large, the function to extract English words is when the pc's ram is low.
# #It may take a long time and may go down.
# 
# X2<-sapply(line, extractNoun,USE.NAMES = T)
# # X2<-sapply(line, extractNoun,USE.NAMES = F)
# 
# #unlist
# c_vector<-unlist(X2)
# 
# #step2) Preprocessing code
# #Special character and blank processing
# c_vector<-str_replace_all(c_vector,"[^[:alpha:]]","")
# 
# #Save only non-blank
# c_vector<-c_vector[c_vector!=""]
# 
# #step3) Check the frequency of desired articles and non-verbs (ìíë ê´ì¬ì ë¹ëì¬ ë¹ë íì¸)
# ###a, the, is, am, are, be
# # table (c_vector) dataframe conversion
# X3<-as.data.frame(table(c_vector))
# X3[X3$c=="a"|X3$c=="is"|X3$c=="the"|X3$c=="am"|X3$c=="are"|X3$c=="be",]
# 
# 
# # [Preprocessing still in progress...]
# ###a, the, is, am, are
# #Convert the articles or non-verbs you want to remove to blanks. (ì ê±°íê³  ì¶ì ê´ì¬ë ë¹ëì¬ë¥¼ ê³µë°±ì¼ë¡ ë³íí¨)
# c_vector<-gsub("is","",c_vector)
# c_vector<-gsub("the","",c_vector)
# c_vector<-gsub("am","",c_vector)
# c_vector<-gsub("are","",c_vector)
# 
# # The Number of Rows/Columns
# NROW(c_vector)
# 
# #Extract only 2 digits or more (2ìë¦¬ì´ìë§ ì¶ì¶)
# c_vector<-Filter(function(x){nchar(x)>2},c_vector)
# 
# 
# # Save as txt (txtë¡ ì ì¥)
# write(c_vector,"ObamaFilter.txt")
# H1<-read.table("ObamaFilter.txt")
# 
# #End of preprocessing (ì ì²ë¦¬ ì¢ë£)
# str(H1)
# wordcount<-table(H1)
# H1
# #step4) Emotion score calculation (ê°ì ì ì ê³ì°)
# results <- score.sentiment(H1$V1, pos.words, neg.words)
# H1$score<-results$score
# H1_mean<-mean(H1$score)
# View(H1_mean)
# 
# 
# H1_mean
# #for example) 0.02071823
# #obama
# 
# #H1_mean = Score of line sentence (H1_mean = line ë¬¸ì¥ì ê°ì ì ì)
# #H1_meanì´ ê°ì ì ìë¼ê³  ë³¼ ì ìì (H1_meanì´ ê°ì ì ìë¼ê³  ë³¼ ì ìì)
# #1 point per 1 positive word-1 point per 1 negative word (ex: ê¸ì ë¨ì´ 1ê°ë¹ 1ì  ë¶ì ë¨ì´ 1ê°ë¹ -1ì )
# #The maximum value is between -1 and 1. (ìµëê°ì -1 ~ 1 ì¬ì´ì ê°ì ê°ì§ê² ë¨)
# 
# 
# # rm(list=ls())
# #Initialization for resource recovery (ë¦¬ìì¤ íë³µì ìí ì´ê¸°í)
# 
# 
# ##Start trump example
# 
# conn=file("trump.txt",open="r")
# line=readLines(conn)
# for (i in 1:length(line)){
#   print(line[i])
# }
# close(conn)
# 
# 
# 
# X2<-sapply(line, extractNoun,USE.NAMES = T)
# 
# 
# c<-unlist(X2)
# 
# 
# 
# X3<-as.data.frame(table(c))
# X3[X3$c=="a"|X3$c=="is"|X3$c=="the"|X3$c=="am"|X3$c=="are"|X3$c=="be",]
# 
# 
# ###a, the, is, am, are, be
# 
# 
# 
# c<-Filter(function(x){nchar(x)>2},c)
# 
# #Special character and blank processing (í¹ìë¬¸ì ê³µë°±ì²ë¦¬)
# #res<-str_replace_all(c,"[^[:alpha:]]","")
# res<-c
# #Save only non-blank (ê³µë°±ì´ ìëê²ë§ ì ì¥)
# res<-res[res!=""]
# 
# ###a, the, is, am, are
# #Convert the articles or non-verbs you want to remove to blanks (ì ê±°íê³  ì¶ì ê´ì¬ë ë¹ëì¬ë¥¼ ê³µë°±ì¼ë¡ ë³íí¨)
# res<-gsub("is","",res)
# res<-gsub("the","",res)
# res<-gsub("am","",res)
# res<-gsub("are","",res)
# 
# NROW(res)
# 
# # If unnecessary words are blanked out and saved as txt,
# # Comes in with no spaces.
# #ë¶íìí ë¨ì´ë¥¼ ê³µë°±ì²ë¦¬íê³  txtë¡ ì ì¥í ë¶ë¬ë¤ì´ë©´ 
# # ê³µë°±ì´ ìì´ì§ì±ë¡ ë¤ì´ì¤ê²ë¨.
# write(res,"Trump_temp.txt")
# H1_Trump<-read.table("Trump_temp.txt")
# 
# 
# str(H1_Trump)
# wordcountTrump<-table(H1_Trump)
# View(H1_Trump)
# 
# 
# #Repeat the score.sentiment function above.
# #ìì ìë score.sentiment í¨ìë¥¼ ë¤ì ìííì¸ì.
# results_Trump <- score.sentiment(H1_Trump$V1, pos.words, neg.words)
# H1_Trump$score<-results_Trump$score
# H1_Tmean<-mean(H1_Trump$score)
# View(H1_Tmean)
# 
# H1_Tmean
# #0.04276069
# #trump
# 
# 
# obama_trump_score<-c(H1_mean,H1_Tmean)
# 
# windows()
# barplot(obama_trump_score, col=rainbow(2), 
#         xlab = "Speaker (Red: Obama, Blue: Trump)", ylab = "Speech affirmation")
# title(main = "Positive comparison (Obama vs. Trump), ê¸ì ë ë¹êµ(Obama vs. Trump)", font = 4)
# 
# #Comparing Obama and Trump's speech, we can see that Trump mentioned more positive words.
# #ì¤ë°ë§ì í¸ë¼íì ì°ì¤ë¬¸ì ë¹êµí ê²°ê³¼ í¸ë¼íê° ê¸ì ì ì¸ë¨ì´ë¥¼ ë ë§ì´ ì¸ê¸í ê²ì ì ì ìì.
# 
# #If you replace it with other celebrities like # steve.txt, bill.txt, etc., you can analyze the same result positively.
# #steve.txt, bill.txt ë±ê³¼ ê°ì´ ë¤ë¥¸ ì ëª ì¸ì¬ë¬¸ì¼ë¡ ë°ê¿ì ë£ì¼ë©´ ëì¼í ê²°ê³¼ë¥¼ ê¸ì ë ë¶ìê°ë¥í©ëë¤.
# 
# #[Team Project][Due date: June. 1st, Monday] Big Data Analysis about "Coronavirus"]
# #Find the coronavirus data among articles in the New York Times site and save it as a txt file and compare the frequency of positive and negative words with respect to the coronavirus.
# 
